tag: 
learn: normal
env: FrankaDrawerEnv
render: True
scenario: 
control_mode: ik_controller
model_path: /home/qiao/RL-Dyn-Env-main/log/sim-4
play_epoch: best
epochs: 200
cycles: 50
num_envs: 10
train_batches: 50
batch_size: 256
timesteps: 80
env_n_substeps: 80
num_obst: 1
gamma: 0.98
pi_lr: 0.001
q_lr: 0.001
act_l2: 1.0
polyak: 0.95
actor_layer_sizes: [512, 256, 128]
actor_batch_norm: [False, False, False]
critic_layer_sizes: [512, 256, 128]
critic_batch_norm: [False, False, False]
obj_lost_reward: -0.3
collision_reward: -0.5
clip_return: True
reward_min: -1.0
reward_max: 0.0
eps_act: 0.3
std_act: 0.2
buffer_size: 20000
buffer_type: energy
her: future
her_ratio: 0.8
pool_rule: full
goal_based: True
cur_acc: 0.0
obs_dims: [82]
acts_dims: [4]
compute_reward: <bound method FrankaDrawerEnv.compute_reward of <envs.franka_drawer_env.FrankaDrawerEnv object at 0x7f4b15846850>>
*** agent initialization complete ***
*** tester initialization complete ***
